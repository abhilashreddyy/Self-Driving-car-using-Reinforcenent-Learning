{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training self driving car on TD3 deep reinforcement learning algorithm \n",
    "\n",
    "## Summary of what is done \n",
    "- Added CNN to calculate state from image\n",
    "    - replaced the sensor input with cropped & rotated Image input\n",
    "- Inputting the orientation of the car to neural network\n",
    "    - __NOTE__ : Please refer to Actor and Critic Images to understand better\n",
    "    - Without orientation car was stumbling here and therewhile staying on road. \n",
    "    - So added orientation to acknowledge the agent to reach destiny\n",
    "- Shifted the entire update operation to car.py from brain.update()\n",
    "- removed tanh activation __to stop__ the __ghumr effect__\n",
    "- reduced the LR of the optmiser\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "## Observations\n",
    "- After training with this code car was able to understand how to keep itself on roads. But was unable to learn how to reach destiny\n",
    "- tweaking the reward and environment and done conditions should give better results\\\n",
    "__NOTE__ : I dont have GPU so I was unable to do much hyper parameter tuning\n",
    "\n",
    "\n",
    "\n",
    "## Improvements (that can be done)\n",
    "- Have done imrovements. with a 2 phase learning please refer to two-phase-learning branch for detailed documentation and code\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "__Refer__ [this](https://youtu.be/A6wUZMdBIzE) link to see some video of how car was training.\n",
    "- These are some small instances of recording while the model was training.\n",
    "- It can be clearly observed that the model is trying move across the destinations but facing a little difficulty in staying on road\n",
    "\n",
    "\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer(object):\n",
    "\n",
    "  def __init__(self, max_size=2e3):\n",
    "    self.storage = []\n",
    "    self.max_size = max_size\n",
    "    self.ptr = 0\n",
    "\n",
    "  def add(self, transition):\n",
    "    if len(self.storage) == self.max_size:\n",
    "      self.storage[int(self.ptr)] = transition\n",
    "      self.ptr = (self.ptr + 1) % self.max_size\n",
    "    else:\n",
    "      self.storage.append(transition)\n",
    "\n",
    "  def sample(self, batch_size):\n",
    "    ind = np.random.randint(0, len(self.storage), size=batch_size)\n",
    "    batch_states, batch_orientation , batch_next_states, batch_next_orientation,  batch_actions, batch_rewards, batch_dones = [], [], [], [], [], [], []\n",
    "    for i in ind:\n",
    "      state, orientation,  next_state, next_orientation, action, reward, done = self.storage[i]\n",
    "      batch_states.append(np.array(state, copy=False))\n",
    "      batch_orientation.append(np.array(orientation, copy=False))\n",
    "      batch_next_states.append(np.array(next_state, copy=False))\n",
    "      batch_next_orientation.append(np.array(next_orientation, copy=False))\n",
    "      batch_actions.append(np.array(action, copy=False))\n",
    "      batch_rewards.append(np.array(reward, copy=False))\n",
    "      batch_dones.append(np.array(done, copy=False))\n",
    "    return np.array(batch_states), np.array(batch_orientation),np.array(batch_next_states), np.array(batch_next_orientation), np.array(batch_actions), np.array(batch_rewards).reshape(-1, 1), np.array(batch_dones).reshape(-1, 1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## actor architecture\n",
    "![actor](image_pres/final_actor.jpg)\n",
    "\n",
    "- observe in the __forward()__ function in am not using tanh. \n",
    "- Because of etrimities of tanh I believe that the agent is tending to rotate at a single position(__ghumr__).\n",
    "- so i have decided to remove the tanh part. __*results were better after removing tanh*__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor(nn.Module):\n",
    "\n",
    "  def __init__(self, state_dim, action_dim, max_action):\n",
    "    super(Actor, self).__init__()\n",
    "    self.convblock1 = nn.Sequential(\n",
    "        nn.Conv2d(in_channels=1, out_channels=8, kernel_size=(3, 3), padding=0, bias=False, stride = 2),\n",
    "        nn.ReLU(),\n",
    "        nn.BatchNorm2d(8)\n",
    "    )\n",
    "    self.convblock2 = nn.Sequential(\n",
    "        nn.Conv2d(in_channels=8, out_channels=16, kernel_size=(3, 3), padding=0, bias=False),\n",
    "        nn.ReLU(),\n",
    "        nn.BatchNorm2d(16)\n",
    "    )\n",
    "    self.pool1 = nn.MaxPool2d(2, 2)\n",
    "    self.convblock3 = nn.Sequential(\n",
    "        nn.Conv2d(in_channels=16, out_channels=16, kernel_size=(3, 3), padding=0, bias=False),\n",
    "        nn.ReLU(),\n",
    "        nn.BatchNorm2d(16)\n",
    "    )\n",
    "    #self.pool2 = nn.MaxPool2d(2,2)\n",
    "    self.convblock4 = nn.Sequential(\n",
    "        nn.Conv2d(in_channels=16, out_channels=8, kernel_size=(3, 3), padding=0, bias=False),\n",
    "        nn.ReLU(),\n",
    "        nn.BatchNorm2d(8)\n",
    "    )\n",
    "    self.pool3 = nn.MaxPool2d(2,2)\n",
    "    self.convblock5 = nn.Sequential(\n",
    "        nn.Conv2d(in_channels=8, out_channels=8, kernel_size=(3, 3), padding=0, bias=False),\n",
    "        nn.ReLU(),\n",
    "        nn.BatchNorm2d(8)\n",
    "    )\n",
    "    self.convblock6 = nn.Sequential(\n",
    "        nn.Conv2d(in_channels=8, out_channels=2, kernel_size=(1, 1), padding=0),\n",
    "        nn.BatchNorm2d(2),\n",
    "        # nn.ReLU() NEVER!\n",
    "    )\n",
    "    self.layer_1 = nn.Linear(18 + 2, 16)\n",
    "    self.layer_2 = nn.Linear(16, 8)\n",
    "    self.layer_3 = nn.Linear(8, action_dim)\n",
    "    self.max_action = max_action\n",
    "\n",
    "\n",
    "  def forward(self, x, o):\n",
    "    x = self.convblock1(x)\n",
    "    x = self.convblock2(x)\n",
    "    x = self.pool1(x)\n",
    "    #x = self.convblock3(x)\n",
    "    #x = self.pool2(x)\n",
    "    x = self.convblock4(x)\n",
    "    x = self.pool3(x)\n",
    "    x = self.convblock5(x)\n",
    "    x = self.convblock6(x)\n",
    "    x = x.view(x.size(0), -1)\n",
    "    x = torch.cat([x, o], 1)\n",
    "    x = F.relu(self.layer_1(x))\n",
    "    x = F.relu(self.layer_2(x))\n",
    "#    x = self.layer_1(x)\n",
    "#    x = self.layer_2(x)\n",
    "    x = self.layer_3(x)\n",
    "#    x = self.max_action * torch.tanh(self.layer_3(x))\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## critic architecture\n",
    "![critic](image_pres/final_critic.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Critic(nn.Module):\n",
    "\n",
    "  def __init__(self, state_dim, action_dim):\n",
    "    super(Critic, self).__init__()\n",
    "    # Defining the first Critic neural network\n",
    "\n",
    "    self.convblock1 = nn.Sequential(\n",
    "        nn.Conv2d(in_channels=1, out_channels=8, kernel_size=(3, 3), padding=0, bias=False, stride = 2),\n",
    "        nn.ReLU(),\n",
    "        nn.BatchNorm2d(8)\n",
    "    )\n",
    "    self.convblock2 = nn.Sequential(\n",
    "        nn.Conv2d(in_channels=8, out_channels=16, kernel_size=(3, 3), padding=0, bias=False),\n",
    "        nn.ReLU(),\n",
    "        nn.BatchNorm2d(16)\n",
    "    )\n",
    "    self.pool1 = nn.MaxPool2d(2, 2)\n",
    "    self.convblock3 = nn.Sequential(\n",
    "        nn.Conv2d(in_channels=16, out_channels=16, kernel_size=(3, 3), padding=0, bias=False),\n",
    "        nn.ReLU(),\n",
    "        nn.BatchNorm2d(16)\n",
    "    )\n",
    "    self.pool2 = nn.MaxPool2d(2, 2)\n",
    "    self.convblock4 = nn.Sequential(\n",
    "        nn.Conv2d(in_channels=16, out_channels=8, kernel_size=(3, 3), padding=0, bias=False),\n",
    "        nn.ReLU(),\n",
    "        nn.BatchNorm2d(8)\n",
    "    )\n",
    "    self.pool3 = nn.MaxPool2d(2,2)\n",
    "    self.convblock5 = nn.Sequential(\n",
    "        nn.Conv2d(in_channels=8, out_channels=8, kernel_size=(3, 3), padding=0, bias=False),\n",
    "        nn.ReLU(),\n",
    "        nn.BatchNorm2d(8)\n",
    "    )\n",
    "    self.convblock6 = nn.Sequential(\n",
    "        nn.Conv2d(in_channels=8, out_channels=2, kernel_size=(1, 1), padding=0,),\n",
    "        nn.BatchNorm2d(2)\n",
    "        # nn.ReLU() NEVER!\n",
    "    )\n",
    "\n",
    "\n",
    "    self.convblock7 = nn.Sequential(\n",
    "        nn.Conv2d(in_channels=1, out_channels=8, kernel_size=(3, 3), padding=0, bias=False, stride = 2),\n",
    "        nn.ReLU(),\n",
    "        nn.BatchNorm2d(8)\n",
    "    )\n",
    "    self.convblock8 = nn.Sequential(\n",
    "        nn.Conv2d(in_channels=8, out_channels=16, kernel_size=(3, 3), padding=0, bias=False),\n",
    "        nn.ReLU(),\n",
    "        nn.BatchNorm2d(16)\n",
    "    )\n",
    "    self.pool4 = nn.MaxPool2d(2, 2)\n",
    "    self.convblock9 = nn.Sequential(\n",
    "        nn.Conv2d(in_channels=16, out_channels=16, kernel_size=(3, 3), padding=0, bias=False),\n",
    "        nn.ReLU(),\n",
    "        nn.BatchNorm2d(16)\n",
    "    )\n",
    "    self.pool5 = nn.MaxPool2d(2, 2)\n",
    "    self.convblock10 = nn.Sequential(\n",
    "        nn.Conv2d(in_channels=16, out_channels=8, kernel_size=(3, 3), padding=0, bias=False),\n",
    "        nn.ReLU(),\n",
    "        nn.BatchNorm2d(8)\n",
    "    )\n",
    "    self.pool6 = nn.MaxPool2d(2,2)\n",
    "    self.convblock11 = nn.Sequential(\n",
    "        nn.Conv2d(in_channels=8, out_channels=8, kernel_size=(3, 3), padding=0, bias=False),\n",
    "        nn.ReLU(),\n",
    "        nn.BatchNorm2d(8)\n",
    "    )\n",
    "    self.convblock12 = nn.Sequential(\n",
    "        nn.Conv2d(in_channels=8, out_channels=2, kernel_size=(1, 1), padding=0),\n",
    "        nn.BatchNorm2d(2)\n",
    "        # nn.ReLU() NEVER!\n",
    "    )\n",
    "\n",
    "    self.layer_1 = nn.Linear(18+2+action_dim, 16)\n",
    "    self.layer_2 = nn.Linear(16, 8)\n",
    "    self.layer_3 = nn.Linear(8, 1)\n",
    "    # Defining the second Critic neural network\n",
    "    self.layer_4 = nn.Linear(18+2+action_dim, 16)\n",
    "    self.layer_5 = nn.Linear(16, 8)\n",
    "    self.layer_6 = nn.Linear(8, 1)\n",
    "\n",
    "  def forward(self, x, o, u):\n",
    "    #print(\"x : \", x)\n",
    "    #print(\"u : \", u)\n",
    "    x1 = self.convblock1(x)\n",
    "    x1 = self.convblock2(x1)\n",
    "    x1 = self.pool1(x1)\n",
    "    #x1 = self.convblock3(x1)\n",
    "    #x1 = self.pool2(x1)\n",
    "    x1 = self.convblock4(x1)\n",
    "    x1 = self.pool3(x1)\n",
    "    x1 = self.convblock5(x1)\n",
    "    x1 = self.convblock6(x1)\n",
    "    x1 = x1.view( x1.size(0), -1)\n",
    "    x1o = torch.cat([x1,o], 1)\n",
    "    x1u = torch.cat([x1o, u], 1)\n",
    "    # Forward-Propagation on the first Critic Neural Network\n",
    "    x1 = F.relu(self.layer_1(x1u))\n",
    "    x1 = F.relu(self.layer_2(x1))\n",
    "    x1 = self.layer_3(x1)\n",
    "    # Forward-Propagation on the second Critic Neural Network\n",
    "    x2 = self.convblock7(x)\n",
    "    x2 = self.convblock8(x2)\n",
    "    x2 = self.pool4(x2)\n",
    "    #x2 = self.convblock9(x2)\n",
    "    #x2 = self.pool5(x2)\n",
    "    x2 = self.convblock10(x2)\n",
    "    x2 = self.pool6(x2)\n",
    "    x2 = self.convblock11(x2)\n",
    "    x2 = self.convblock12(x2)\n",
    "    x2 = x2.view( x2.size(0), -1)\n",
    "    x2o = torch.cat([x2,o], 1)\n",
    "    x2u = torch.cat([x2o, u], 1)\n",
    "    x2 = F.relu(self.layer_4(x2u))\n",
    "    x2 = F.relu(self.layer_5(x2))\n",
    "    x2 = self.layer_6(x2)\n",
    "    return x1, x2\n",
    "\n",
    "  def Q1(self, x, o, u):\n",
    "    x1 = self.convblock1(x)\n",
    "    x1 = self.convblock2(x1)\n",
    "    x1 = self.pool1(x1)\n",
    "    #x1 = self.convblock3(x1)\n",
    "    x1 = self.convblock4(x1)\n",
    "    x1 = self.pool1(x1)\n",
    "    x1 = self.convblock5(x1)\n",
    "    x1 = self.convblock6(x1)\n",
    "    x1 = x1.view( x1.size(0), -1)\n",
    "    x1o = torch.cat([x1,o], 1)\n",
    "    x1u = torch.cat([x1o, u], 1)\n",
    "    # Forward-Propagation on the first Critic Neural Network\n",
    "    x1 = F.relu(self.layer_1(x1u))\n",
    "    x1 = F.relu(self.layer_2(x1))\n",
    "    x1 = self.layer_3(x1)\n",
    "\n",
    "    return x1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Building the whole Training Process into a class\n",
    "\n",
    "class TD3(object):\n",
    "\n",
    "  def __init__(self, state_dim, action_dim, max_action):\n",
    "    self.actor = Actor(state_dim, action_dim, max_action).to(device)\n",
    "    self.actor_target = Actor(state_dim, action_dim, max_action).to(device)\n",
    "    self.actor_target.load_state_dict(self.actor.state_dict())\n",
    "    self.actor_optimizer = torch.optim.Adam(self.actor.parameters(), lr = 0.0003)\n",
    "    self.critic = Critic(state_dim, action_dim).to(device)\n",
    "    self.critic_target = Critic(state_dim, action_dim).to(device)\n",
    "    self.critic_target.load_state_dict(self.critic.state_dict())\n",
    "    self.critic_optimizer = torch.optim.Adam(self.critic.parameters(), lr = 0.0003)\n",
    "    self.max_action = max_action\n",
    "\n",
    "  def select_action(self, state, orientation):\n",
    "    # state input for list state\n",
    "    #state = torch.Tensor(state.reshape(1, -1)).to(device)\n",
    "\n",
    "    # state input for image state\n",
    "    state = torch.Tensor(state).unsqueeze(0).unsqueeze(0).to(device)\n",
    "    orientation = torch.Tensor(orientation).unsqueeze(0).to(device)\n",
    "    return self.actor(state, orientation).cpu().data.numpy().flatten()\n",
    "\n",
    "  def train(self, replay_buffer, iterations, batch_size=100, discount=0.99, tau=0.005, policy_noise=0.2, noise_clip=0.5, policy_freq=2):\n",
    "\n",
    "    for it in range(iterations):\n",
    "\n",
    "      # Step 4: We sample a batch of transitions (s, s’, a, r) from the memory\n",
    "      batch_states, batch_orientation, batch_next_states, batch_next_orientation, batch_actions, batch_rewards, batch_dones = replay_buffer.sample(batch_size)\n",
    "      #for cnn state calc\n",
    "      state = torch.Tensor(batch_states).unsqueeze(1).to(device)\n",
    "      next_state = torch.Tensor(batch_next_states).unsqueeze(1).to(device)\n",
    "      # for cnn state calc\n",
    "\n",
    "      #for sensor state calc\n",
    "      # state = torch.Tensor(batch_states).to(device)\n",
    "      # next_state = torch.Tensor(batch_next_states).to(device)\n",
    "      # for sensor state calc\n",
    "\n",
    "      orientation = torch.Tensor(batch_orientation).to(device)\n",
    "      #print(\"orientation shape : \",orientation.shape)\n",
    "      next_orientation = torch.Tensor(batch_next_orientation).to(device)\n",
    "      action = torch.Tensor(batch_actions).to(device)\n",
    "      reward = torch.Tensor(batch_rewards).to(device)\n",
    "      done = torch.Tensor(batch_dones).to(device)\n",
    "\n",
    "\n",
    "\n",
    "      # Step 5: From the next state s’, the Actor target plays the next action a’\n",
    "      next_action = self.actor_target(next_state, next_orientation)\n",
    "\n",
    "      # Step 6: We add Gaussian noise to this next action a’ and we clamp it in a range of values supported by the environment\n",
    "      noise = torch.Tensor(batch_actions).data.normal_(0, policy_noise).to(device)\n",
    "      noise = noise.clamp(-noise_clip, noise_clip)\n",
    "      next_action = (next_action + noise).clamp(-self.max_action, self.max_action)\n",
    "\n",
    "      # Step 7: The two Critic targets take each the couple (s’, a’) as input and return two Q-values Qt1(s’,a’) and Qt2(s’,a’) as outputs\n",
    "      target_Q1, target_Q2 = self.critic_target(next_state, next_orientation,  next_action)\n",
    "\n",
    "      # Step 8: We keep the minimum of these two Q-values: min(Qt1, Qt2)\n",
    "      target_Q = torch.min(target_Q1, target_Q2)\n",
    "\n",
    "      # Step 9: We get the final target of the two Critic models, which is: Qt = r + γ * min(Qt1, Qt2), where γ is the discount factor\n",
    "      target_Q = reward + ((1 - done) * discount * target_Q).detach()\n",
    "\n",
    "      # Step 10: The two Critic models take each the couple (s, a) as input and return two Q-values Q1(s,a) and Q2(s,a) as outputs\n",
    "      current_Q1, current_Q2 = self.critic(state, orientation, action)\n",
    "\n",
    "      # Step 11: We compute the loss coming from the two Critic models: Critic Loss = MSE_Loss(Q1(s,a), Qt) + MSE_Loss(Q2(s,a), Qt)\n",
    "      critic_loss = F.mse_loss(current_Q1, target_Q) + F.mse_loss(current_Q2, target_Q)\n",
    "\n",
    "      # Step 12: We backpropagate this Critic loss and update the parameters of the two Critic models with a SGD optimizer\n",
    "      self.critic_optimizer.zero_grad()\n",
    "      critic_loss.backward()\n",
    "      self.critic_optimizer.step()\n",
    "\n",
    "      # Step 13: Once every two iterations, we update our Actor model by performing gradient ascent on the output of the first Critic model\n",
    "      if it % policy_freq == 0:\n",
    "        # actor_loss,_ = -self.critic(state, orientation, self.actor(state))\n",
    "        # actor_loss = actor_loss.mean()\n",
    "        #print(\"debug : \",state.shape, type(state), orientation.shape, type(orientation), self.actor(state).shape, type(self.actor(state)))\n",
    "        actor_loss = -self.critic.Q1(state, orientation, self.actor(state, orientation)).mean()\n",
    "        self.actor_optimizer.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        self.actor_optimizer.step()\n",
    "\n",
    "        # Step 14: Still once every two iterations, we update the weights of the Actor target by polyak averaging\n",
    "        for param, target_param in zip(self.actor.parameters(), self.actor_target.parameters()):\n",
    "          target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)\n",
    "\n",
    "        # Step 15: Still once every two iterations, we update the weights of the Critic target by polyak averaging\n",
    "        for param, target_param in zip(self.critic.parameters(), self.critic_target.parameters()):\n",
    "          target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)\n",
    "\n",
    "  # Making a save method to save a trained model\n",
    "  def save(self, filename = \"temp\", directory = \"models\"):\n",
    "    torch.save(self.actor.state_dict(), '%s/%s_actor.pth' % (directory, filename))\n",
    "    torch.save(self.critic.state_dict(), '%s/%s_critic.pth' % (directory, filename))\n",
    "\n",
    "  # Making a load method to load a pre-trained model\n",
    "  def load(self, filename = \"temp\", directory = \"models\"):\n",
    "    self.actor.load_state_dict(torch.load('%s/%s_actor.pth' % (directory, filename)))\n",
    "    self.critic.load_state_dict(torch.load('%s/%s_critic.pth' % (directory, filename)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Started KIWI environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO   ] [Logger      ] Record log in C:\\Users\\abhi\\.kivy\\logs\\kivy_20-05-08_1.txt\n",
      "[INFO   ] [Kivy        ] v1.11.1\n",
      "[INFO   ] [Kivy        ] Installed at \"C:\\Users\\abhi\\Anaconda3\\lib\\site-packages\\kivy\\__init__.py\"\n",
      "[INFO   ] [Python      ] v3.6.9 |Anaconda, Inc.| (default, Jul 30 2019, 14:00:49) [MSC v.1915 64 bit (AMD64)]\n",
      "[INFO   ] [Python      ] Interpreter at \"C:\\Users\\abhi\\Anaconda3\\python.exe\"\n",
      "[INFO   ] [Factory     ] 184 symbols loaded\n",
      "[INFO   ] [Image       ] Providers: img_tex, img_dds, img_sdl2, img_pil, img_gif (img_ffpyplayer ignored)\n",
      "[INFO   ] [Text        ] Provider: sdl2\n",
      "[INFO   ] [Window      ] Provider: sdl2\n",
      "[INFO   ] [GL          ] Using the \"OpenGL\" graphics system\n",
      "[INFO   ] [GL          ] GLEW initialization succeeded\n",
      "[INFO   ] [GL          ] Backend used <glew>\n",
      "[INFO   ] [GL          ] OpenGL version <b'4.4.0 - Build 21.20.16.4664'>\n",
      "[INFO   ] [GL          ] OpenGL vendor <b'Intel'>\n",
      "[INFO   ] [GL          ] OpenGL renderer <b'Intel(R) HD Graphics 620'>\n",
      "[INFO   ] [GL          ] OpenGL parsed version: 4, 4\n",
      "[INFO   ] [GL          ] Shading version <b'4.40 - Build 21.20.16.4664'>\n",
      "[INFO   ] [GL          ] Texture max size <16384>\n",
      "[INFO   ] [GL          ] Texture max units <32>\n",
      "[INFO   ] [Window      ] auto add sdl2 input provider\n",
      "[INFO   ] [Window      ] virtual keyboard not allowed, single mode, not docked\n",
      "[INFO   ] [GL          ] NPOT texture support is available\n",
      "[INFO   ] [Base        ] Start application main loop\n"
     ]
    }
   ],
   "source": [
    "# Self Driving Car\n",
    "\n",
    "# Importing the libraries\n",
    "import numpy as np\n",
    "from random import random, randint\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "# Importing the Kivy packages\n",
    "from kivy.app import App\n",
    "from kivy.uix.widget import Widget\n",
    "from kivy.uix.image import Image\n",
    "from kivy.uix.button import Button\n",
    "from kivy.graphics import Color, Ellipse, Line\n",
    "from kivy.config import Config\n",
    "from kivy.properties import NumericProperty, ReferenceListProperty, ObjectProperty\n",
    "from kivy.vector import Vector\n",
    "from kivy.clock import Clock\n",
    "from kivy.core.image import Image as CoreImage\n",
    "from PIL import Image as PILImage\n",
    "from kivy.graphics.texture import Texture\n",
    "\n",
    "# Importing the Dqn object from our AI in ai.py\n",
    "#from aiT3D import TD3, ReplayBuffer\n",
    "import random\n",
    "import cv2\n",
    "from scipy import ndimage\n",
    "#from PIL import Image\n",
    "import scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding this line if we don't want the right click to put a red point\n",
    "Config.set('input', 'mouse', 'mouse,multitouch_on_demand')\n",
    "Config.set('graphics', 'resizable', False)\n",
    "Config.set('graphics', 'width', '1429')\n",
    "Config.set('graphics', 'height', '660')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model parameters START\n",
    "seed = 0 # Random seed number\n",
    "start_timesteps = 9e2 # Number of iterations/timesteps before which the model randomly chooses an action, and after which it starts to use the policy network\n",
    "eval_freq = 5e2 # How often the evaluation step is performed (after how many timesteps)\n",
    "#max_timesteps = 5e5 # Total number of iterations/timesteps\n",
    "save_models = True # Boolean checker whether or not to save the pre-trained model\n",
    "expl_noise = 1.0 # Exploration noise - STD value of exploration Gaussian noise\n",
    "batch_size = 30 # Size of the batch\n",
    "discount = 0.99 # Discount factor gamma, used in the calculation of the total discounted reward\n",
    "tau = 0.005 # Target network update rate\n",
    "policy_noise = 0.2 # STD of Gaussian noise added to the actions for the exploration purposes\n",
    "noise_clip = 0.5 # Maximum value of the Gaussian noise added to the actions (policy)\n",
    "policy_freq = 2 # Number of iterations to wait before the policy network (Actor model) is updated\n",
    "done = True\n",
    "total_timesteps = 0\n",
    "timesteps_since_eval = 0\n",
    "episode_num = 0\n",
    "episode_reward = 0\n",
    "episode_timesteps = 0\n",
    "reached_dest = 0\n",
    "\n",
    "action_len = 1\n",
    "state_len = 5\n",
    "last_time_steps = 1\n",
    "image_size = 60\n",
    "orientation = -0.9\n",
    "#obs = [0.23,1,1,0.5, -0.5]\n",
    "# model parameters END\n",
    "\n",
    "# model global params\n",
    "replay_buffer = ReplayBuffer()\n",
    "# model global params\n",
    "\n",
    "\n",
    "# Introducing last_x and last_y, used to keep the last point in memory when we draw the sand on the map\n",
    "last_x = 0\n",
    "last_y = 0\n",
    "n_points = 0\n",
    "length = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting our AI, which we call \"brain\", and that contains our neural network that represents our Q-function\n",
    "max_action_agent = 40\n",
    "brain = TD3(state_len,action_len,max_action_agent)\n",
    "action2rotation = [0,5,-5]\n",
    "reward = 0\n",
    "scores = []\n",
    "reward_window = []\n",
    "im = CoreImage(\"./images/MASK1.png\")\n",
    "main_img = cv2.imread('./images/mask.png',0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_cropped_image(img, x, y, name = \"\"):\n",
    "    # print(\"entered\")\n",
    "    # data = np.array(img)# * 255.0\n",
    "    # rescaled = data.astype(np.uint8)\n",
    "    # im = Image.fromarray(rescaled)\n",
    "    # im.save(\"./check/\"+name+ \"_\" + \"your_file\"+str(x) +\"_\"+ str(y) +\".png\")\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to fetch cropped & rotated image\n",
    "\n",
    "- Let \"AxA\" the required shape\n",
    "- Crop the image in a shape of 1.414xA\n",
    "    - because the square_root 2 of side of a square == hypotenuse\n",
    "- Rotate the image\n",
    "- Again crop \"AxA\" shaped required image\n",
    "\n",
    "__Note__ : If the above explanation is confusing. I am just __rotating the image__ in the angle of car in __get_target_image()__ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_target_image(img, angle, center, size, fill_with = 100.0):\n",
    "    angle = angle + 90\n",
    "    center[0] -= 0\n",
    "    img = np.pad(img, size, 'constant', constant_values = fill_with)\n",
    "    init_size = 1.6*size\n",
    "    center[0] += size\n",
    "    center[1] += size\n",
    "    cropped = img[int(center[0]-(init_size/2)) : int(center[0]+(init_size/2)) ,int(center[1]-(init_size/2)): int(center[1]+(init_size/2))]\n",
    "    rotated = ndimage.rotate(cropped, angle, reshape = False, cval = 255.0)\n",
    "    y,x = rotated.shape\n",
    "    final = rotated[int(y/2-(size/2)):int(y/2+(size/2)),int(x/2-(size/2)):int(x/2+(size/2))]\n",
    "    return cropped, rotated, final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_update = True\n",
    "def init():\n",
    "    global sand\n",
    "    global goal_x\n",
    "    global goal_y\n",
    "    global first_update\n",
    "    sand = np.zeros((longueur,largeur))\n",
    "    img = PILImage.open(\"./images/mask.png\").convert('L')\n",
    "    sand = np.asarray(img)/255\n",
    "    goal_x = 1420\n",
    "    goal_y = 622\n",
    "    first_update = False\n",
    "    global swap\n",
    "    swap = 0\n",
    "\n",
    "\n",
    "# Initializing the last distance\n",
    "last_distance = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the car class\n",
    "\n",
    "class Car(Widget):\n",
    "\n",
    "    angle = NumericProperty(0)\n",
    "    rotation = NumericProperty(0)\n",
    "    velocity_x = NumericProperty(0)\n",
    "    velocity_y = NumericProperty(0)\n",
    "    velocity = ReferenceListProperty(velocity_x, velocity_y)\n",
    "    sensor1_x = NumericProperty(0)\n",
    "    sensor1_y = NumericProperty(0)\n",
    "    sensor1 = ReferenceListProperty(sensor1_x, sensor1_y)\n",
    "    sensor2_x = NumericProperty(0)\n",
    "    sensor2_y = NumericProperty(0)\n",
    "    sensor2 = ReferenceListProperty(sensor2_x, sensor2_y)\n",
    "    sensor3_x = NumericProperty(0)\n",
    "    sensor3_y = NumericProperty(0)\n",
    "    sensor3 = ReferenceListProperty(sensor3_x, sensor3_y)\n",
    "    signal1 = NumericProperty(0)\n",
    "    signal2 = NumericProperty(0)\n",
    "    signal3 = NumericProperty(0)\n",
    "\n",
    "    def move(self, rotation):\n",
    "        self.pos = Vector(*self.velocity) + self.pos\n",
    "        self.rotation = rotation\n",
    "        self.angle = self.angle + self.rotation\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This section of  code is long. So, mentioned the explanation as comments in between code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Game(Widget):\n",
    "\n",
    "    car = ObjectProperty(None)\n",
    "    ball1 = ObjectProperty(None)\n",
    "    ball2 = ObjectProperty(None)\n",
    "    ball3 = ObjectProperty(None)\n",
    "\n",
    "    def serve_car(self):\n",
    "        # randomly choose points to initialise agent\n",
    "        #my_rand_points = [((715, 360),0),((348,414),90),((127,350),95),((581,432),270),((882,71),20),((970,278),0)]\n",
    "        # for simplicity have used a single point\n",
    "        my_rand_points = [((715, 360),0)]\n",
    "        (x,y),angle = random.choice(my_rand_points)\n",
    "        self.car.center = (x,y)\n",
    "        self.car.angle = angle\n",
    "        self.car.velocity = Vector(4, 0)\n",
    "\n",
    "\n",
    "    def update(self, dt):\n",
    "\n",
    "        global brain\n",
    "        global reward\n",
    "        global scores\n",
    "        global last_distance\n",
    "        global goal_x\n",
    "        global goal_y\n",
    "        global longueur\n",
    "        global largeur\n",
    "        global swap\n",
    "        global orientation\n",
    "\n",
    "\n",
    "        global obs\n",
    "\n",
    "\n",
    "        # NEW GLOBALS\n",
    "        global replay_buffer\n",
    "        global seed\n",
    "        global start_timesteps\n",
    "        global eval_freq\n",
    "        #global max_timesteps\n",
    "        global save_models\n",
    "        global expl_noise\n",
    "        global batch_size\n",
    "        global discount\n",
    "        global tau\n",
    "        global policy_noise\n",
    "        global noise_clip\n",
    "        global policy_freq\n",
    "        global done\n",
    "        global total_timesteps\n",
    "        global timesteps_since_eval\n",
    "        global episode_num\n",
    "        global episode_reward\n",
    "        global reward_window\n",
    "\n",
    "        global episode_timesteps\n",
    "        global main_img\n",
    "        global image_size\n",
    "        global reached_dest\n",
    "        global last_time_steps\n",
    "        # NEW GLOBALS\n",
    "\n",
    "\n",
    "        longueur = self.width\n",
    "        largeur = self.height\n",
    "        if first_update:\n",
    "            init()\n",
    "\n",
    "        if True :\n",
    "          # If the episode is done\n",
    "          if done:\n",
    "            # If we are not at the very beginning, we start the training process of the model\n",
    "            if total_timesteps != 0:\n",
    "              print(\"Total Timesteps: {} Episode Num: {} Timesteps diff: {} Reward: {} score: {}\".format(total_timesteps, episode_num, total_timesteps - last_time_steps,episode_reward, episode_reward/(total_timesteps - last_time_steps)))\n",
    "              brain.train(replay_buffer, episode_timesteps, batch_size, discount, tau, policy_noise, noise_clip, policy_freq)\n",
    "              last_time_steps = total_timesteps\n",
    "            \n",
    "            # if the agent have not reached destination and collided to walls then reinitialise the agent at a given point\n",
    "            if  not  reached_dest:\n",
    "                # initialize the car at new point\n",
    "                self.serve_car()\n",
    "                \n",
    "                #cnn state calculation\n",
    "                _,_,obs = get_target_image(main_img, self.car.angle, [self.car.x, self.car.y], image_size)\n",
    "                save_cropped_image(obs, self.car.x, self.car.y, name = \"initial\")\n",
    "                xx = goal_x - self.car.x\n",
    "                yy = goal_y - self.car.y\n",
    "                orientation = Vector(*self.car.velocity).angle((xx,yy))/180.\n",
    "                orientation = [orientation, -orientation]\n",
    "                #cnn state calculation\n",
    "\n",
    "\n",
    "            # Set the Done to False\n",
    "            done = False\n",
    "            # Set rewards and episode timesteps to zero\n",
    "            episode_reward = 0\n",
    "            episode_timesteps = 0\n",
    "            episode_num += 1\n",
    "            reached_dest = 0\n",
    "\n",
    "          # Before start_timesteps, we play random actions\n",
    "          if total_timesteps < start_timesteps:\n",
    "            action = [random.uniform(-max_action_agent * 1.0, max_action_agent * 1.0)]\n",
    "          else: # After start_timesteps timesteps, we switch to the model\n",
    "            action = brain.select_action(np.array(obs), np.array(orientation))\n",
    "            # If the explore_noise parameter is not 0, we add noise to the action and we clip it\n",
    "            if expl_noise != 0:\n",
    "              action = (action + np.random.normal(0, expl_noise, size=action_len)).clip(-1*max_action_agent,max_action_agent)\n",
    "\n",
    "          # The agent performs the action in the environment, then reaches the next state and receives the reward\n",
    "\n",
    "\n",
    "          # ENV STEP PERFORM START\n",
    "          if type(action) != type([]):\n",
    "              self.car.move(action.tolist()[0])\n",
    "          else:\n",
    "              self.car.move(action[0])\n",
    "          distance = np.sqrt((self.car.x - goal_x)**2 + (self.car.y - goal_y)**2)\n",
    "\n",
    "\n",
    "          if sand[int(self.car.x),int(self.car.y)] > 0:\n",
    "              self.car.velocity = Vector(1, 0).rotate(self.car.angle)\n",
    "\n",
    "              reward = -0.5\n",
    "              if distance < last_distance:\n",
    "                reward = -0.2\n",
    "          else: # otherwise\n",
    "              self.car.velocity = Vector(2, 0).rotate(self.car.angle)\n",
    "              reward = -0.22\n",
    "              if distance < last_distance:\n",
    "                  reward = 0.1\n",
    "              # else:\n",
    "              #     last_reward = last_reward +(-0.2)\n",
    "\n",
    "\n",
    "          # in the below code elif condition refers to when the car reaches the walls then the reward starts increaseing even before the 5 steps to walls\n",
    "          if self.car.x < 5:\n",
    "              reward_window.append(-3)\n",
    "              self.car.x = 5\n",
    "              reward = -1\n",
    "          elif self.car.x < 10:\n",
    "              reward = -1# * (10-self.car.x)\n",
    "              \n",
    "          if self.car.x > self.width - 5:\n",
    "              reward_window.append(-3)\n",
    "              self.car.x = self.width - 5\n",
    "              reward = -0.7\n",
    "          elif self.car.x > self.width - 10:\n",
    "              reward = -0.1 * (self.car.x- self.width +10)\n",
    "              \n",
    "          if self.car.y < 5:\n",
    "              reward_window.append(-3)\n",
    "              self.car.y = 5\n",
    "              reward = -1\n",
    "          elif self.car.y < 10:\n",
    "              reward = -1# * (10-self.car.y)\n",
    "              \n",
    "          if self.car.y > self.height - 5:\n",
    "              reward_window.append(-3)\n",
    "              self.car.y = self.height - 5\n",
    "              reward = -1\n",
    "          elif self.car.y > self.height - 10:\n",
    "              reward = -1# * (self.car.y- self.height+10)\n",
    "\n",
    "          # when the car reaches near the destination\n",
    "          if distance < 25:\n",
    "              done = True\n",
    "              reached_dest = 1\n",
    "              if swap == 1:\n",
    "                  goal_x = 1420\n",
    "                  goal_y = 622\n",
    "                  swap = 0\n",
    "              else:\n",
    "                  goal_x = 9\n",
    "                  goal_y = 85\n",
    "                  swap = 1\n",
    "              reward = 2\n",
    "          last_distance = distance\n",
    "\n",
    "          # cnn state calculation\n",
    "          _,_,new_obs = get_target_image(main_img, self.car.angle, [self.car.x, self.car.y], image_size)\n",
    "          xx = goal_x - self.car.x\n",
    "          yy = goal_y - self.car.y\n",
    "          new_orientation = Vector(*self.car.velocity).angle((xx,yy))/180.\n",
    "          new_orientation = [new_orientation, -new_orientation]\n",
    "          save_cropped_image(new_obs, self.car.x, self.car.y, name = \"\")\n",
    "          # cnn state calculation\n",
    "\n",
    "       \n",
    "\n",
    "          reward_window.append(reward)\n",
    "        \n",
    "          ## set the done condition if the reward reaches a particular negative treshold in previous N steps\n",
    "          ## set the done condition after every 1200 steps\n",
    "          if sum(reward_window[len(reward_window)-100:]) <= -188 or episode_timesteps % 1200 == 0 and episode_timesteps != 0:\n",
    "              done = True\n",
    "              reward_window = []\n",
    "\n",
    "\n",
    "          # ENV STEP PERFORM END\n",
    "\n",
    "        \n",
    "          # We increase the total reward\n",
    "          episode_reward += reward\n",
    "\n",
    "          # We store the new transition into the Experience Replay memory (ReplayBuffer)\n",
    "          replay_buffer.add((obs, orientation, new_obs, new_orientation, action, reward, done))\n",
    "\n",
    "          # We update the state, the episode timestep, the total timesteps, and the timesteps since the evaluation of the policy\n",
    "          obs = new_obs\n",
    "          orientation = new_orientation\n",
    "          episode_timesteps += 1\n",
    "          total_timesteps += 1\n",
    "          timesteps_since_eval += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyPaintWidget(Widget):\n",
    "\n",
    "    def on_touch_down(self, touch):\n",
    "        global length, n_points, last_x, last_y\n",
    "        with self.canvas:\n",
    "            Color(0.8,0.7,0)\n",
    "            d = 10.\n",
    "            touch.ud['line'] = Line(points = (touch.x, touch.y), width = 10)\n",
    "            last_x = int(touch.x)\n",
    "            last_y = int(touch.y)\n",
    "            n_points = 0\n",
    "            length = 0\n",
    "            sand[int(touch.x),int(touch.y)] = 1\n",
    "            img = PILImage.fromarray(sand.astype(\"uint8\")*255)\n",
    "            img.save(\"./images/sand.jpg\")\n",
    "\n",
    "    def on_touch_move(self, touch):\n",
    "        global length, n_points, last_x, last_y\n",
    "        if touch.button == 'left':\n",
    "            touch.ud['line'].points += [touch.x, touch.y]\n",
    "            x = int(touch.x)\n",
    "            y = int(touch.y)\n",
    "            length += np.sqrt(max((x - last_x)**2 + (y - last_y)**2, 2))\n",
    "            n_points += 1.\n",
    "            density = n_points/(length)\n",
    "            touch.ud['line'].width = int(20 * density + 1)\n",
    "            sand[int(touch.x) - 10 : int(touch.x) + 10, int(touch.y) - 10 : int(touch.y) + 10] = 1\n",
    "\n",
    "\n",
    "            last_x = x\n",
    "            last_y = y\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CarApp(App):\n",
    "\n",
    "    def build(self):\n",
    "        parent = Game()\n",
    "        parent.serve_car()\n",
    "        Clock.schedule_interval(parent.update, 1.0/60.0)\n",
    "        self.painter = MyPaintWidget()\n",
    "        clearbtn = Button(text = 'clear')\n",
    "        savebtn = Button(text = 'save', pos = (parent.width, 0))\n",
    "        loadbtn = Button(text = 'load', pos = (2 * parent.width, 0))\n",
    "        clearbtn.bind(on_release = self.clear_canvas)\n",
    "        savebtn.bind(on_release = self.save)\n",
    "        loadbtn.bind(on_release = self.load)\n",
    "        marker_home = Image(source = \"./images/home.jpg\", pos = (1340,582))\n",
    "        marker_offce = Image(source = \"./images/office.jpg\", pos = (9,85))\n",
    "        parent.add_widget(self.painter)\n",
    "        parent.add_widget(clearbtn)\n",
    "        parent.add_widget(savebtn)\n",
    "        parent.add_widget(loadbtn)\n",
    "        parent.add_widget(marker_home)\n",
    "        parent.add_widget(marker_offce)\n",
    "        return parent\n",
    "\n",
    "    def clear_canvas(self, obj):\n",
    "        global sand\n",
    "        self.painter.canvas.clear()\n",
    "        sand = np.zeros((longueur,largeur))\n",
    "\n",
    "    def save(self, obj):\n",
    "        print(\"saving brain...\")\n",
    "        brain.save()\n",
    "        plt.plot(scores)\n",
    "        plt.show()\n",
    "\n",
    "    def load(self, obj):\n",
    "        print(\"loading last saved brain...\")\n",
    "        brain.load()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Timesteps: 1201 Episode Num: 1 Timesteps diff: 1200 Reward: -977.100000000004 score: -0.8142500000000034\n"
     ]
    }
   ],
   "source": [
    "# Running the whole thing\n",
    "if __name__ == '__main__':\n",
    "    CarApp().run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
